{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e3a20a41",
            "metadata": {},
            "source": [
                "# 第五次课作业：\n",
                "\n",
                "作业一：检索结果的后处理方法\n",
                "\n",
                "本次作业旨在帮助大家深入掌握：\n",
                "1. **重排技术（Reranking）**：对初步检索结果进行重新排序，提升相关性。\n",
                "2. **压缩技术（Compression）**：减少检索结果的冗余，提取关键信息。\n",
                "3. **校正技术（Correction）**：优化查询本身，改善检索质量。"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd46bc36",
            "metadata": {},
            "source": [
                "## 任务一：重排技术（Reranking）\n",
                "\n",
                "### 1.1 环境准备\n",
                "安装必要的库，包括向量检索、重排序和语言模型相关工具。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "44768ef9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
                        "Requirement already satisfied: sentence-transformers in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (3.0.1)\n",
                        "Requirement already satisfied: chromadb in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (0.5.3)\n",
                        "Requirement already satisfied: rank-bm25 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (0.2.2)\n",
                        "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (0.3.8)\n",
                        "Requirement already satisfied: langchain-community in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (0.3.8)\n",
                        "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (4.44.2)\n",
                        "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (4.66.5)\n",
                        "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (2.2.2)\n",
                        "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (1.26.4)\n",
                        "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (1.5.1)\n",
                        "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (1.14.1)\n",
                        "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (0.24.6)\n",
                        "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
                        "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (3.15.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (24.1)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.7.24)\n",
                        "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2.32.3)\n",
                        "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
                        "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
                        "Requirement already satisfied: build>=1.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
                        "Requirement already satisfied: pydantic>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (2.8.2)\n",
                        "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
                        "Requirement already satisfied: fastapi>=0.95.2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.112.2)\n",
                        "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
                        "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (3.5.2)\n",
                        "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.20.0)\n",
                        "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
                        "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.47b0)\n",
                        "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
                        "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
                        "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
                        "Requirement already satisfied: importlib-resources in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (6.4.4)\n",
                        "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (1.63.0)\n",
                        "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (4.2.0)\n",
                        "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.12.5)\n",
                        "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (30.1.0)\n",
                        "Requirement already satisfied: tenacity>=8.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (8.5.0)\n",
                        "Requirement already satisfied: mmh3>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
                        "Requirement already satisfied: orjson>=3.9.12 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (3.10.7)\n",
                        "Requirement already satisfied: httpx>=0.27.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from chromadb) (0.27.2)\n",
                        "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain) (2.0.32)\n",
                        "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain) (3.10.5)\n",
                        "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain) (0.3.21)\n",
                        "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain) (0.3.2)\n",
                        "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain) (0.1.145)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
                        "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
                        "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
                        "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
                        "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
                        "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
                        "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
                        "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.8)\n",
                        "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
                        "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.20.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.3.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2.2.2)\n",
                        "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
                        "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
                        "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from langchain-community) (2.6.1)\n",
                        "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
                        "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
                        "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
                        "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
                        "Requirement already satisfied: pyproject_hooks in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
                        "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.38.2)\n",
                        "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
                        "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.34.0)\n",
                        "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
                        "Requirement already satisfied: requests-oauthlib in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
                        "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
                        "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
                        "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
                        "Requirement already satisfied: coloredlogs in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
                        "Requirement already satisfied: flatbuffers in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
                        "Requirement already satisfied: protobuf in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
                        "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
                        "Requirement already satisfied: deprecated>=1.2.6 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
                        "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
                        "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.1)\n",
                        "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.16.0)\n",
                        "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
                        "Requirement already satisfied: opentelemetry-proto==1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
                        "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.47b0)\n",
                        "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
                        "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
                        "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
                        "Requirement already satisfied: setuptools>=16.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (80.9.0)\n",
                        "Requirement already satisfied: asgiref~=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
                        "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
                        "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
                        "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
                        "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
                        "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
                        "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (13.8.0)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
                        "Requirement already satisfied: httptools>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
                        "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
                        "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.0)\n",
                        "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0)\n",
                        "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install sentence-transformers chromadb rank-bm25 langchain langchain-community"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b255025",
            "metadata": {},
            "source": [
                "### 1.2 准备测试数据\n",
                "创建一个简单的文档集合用于演示后处理技术。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "12d47d73",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from tqdm.autonotebook import tqdm, trange\n",
                        "/opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "已添加 10 个文档到向量数据库\n"
                    ]
                }
            ],
            "source": [
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# 准备文档集合\n",
                "documents = [\n",
                "    \"大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。\",\n",
                "    \"ChatGPT 是 OpenAI 开发的对话式 AI 模型，基于 GPT-3.5 和 GPT-4 架构。\",\n",
                "    \"向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。\",\n",
                "    \"检索增强生成（RAG）结合了信息检索和文本生成，可以提供更准确的答案。\",\n",
                "    \"Transformer 架构由 Google 在 2017 年提出，彻底改变了自然语言处理领域。\",\n",
                "    \"BERT 是一种双向编码器模型，适用于各种 NLP 任务如文本分类和命名实体识别。\",\n",
                "    \"向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。\",\n",
                "    \"深度学习需要大量的训练数据和计算资源，GPU 是常用的加速硬件。\",\n",
                "    \"Python 是机器学习和数据科学领域最流行的编程语言之一。\",\n",
                "    \"北京是中国的首都，拥有悠久的历史和丰富的文化遗产。\"\n",
                "]\n",
                "\n",
                "# 初始化 ChromaDB\n",
                "client = chromadb.Client()\n",
                "default_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                "    model_name=\"all-MiniLM-L6-v2\"\n",
                ")\n",
                "\n",
                "# 创建 collection\n",
                "collection = client.create_collection(\n",
                "    name=\"rag_documents\",\n",
                "    embedding_function=default_ef\n",
                ")\n",
                "\n",
                "# 添加文档\n",
                "collection.add(\n",
                "    documents=documents,\n",
                "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
                ")\n",
                "\n",
                "print(f\"已添加 {len(documents)} 个文档到向量数据库\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52d2035f",
            "metadata": {},
            "source": [
                "### 1.3 基础向量检索\n",
                "首先执行基础的向量相似度检索，这是后处理的起点。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "45a6ad3c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "查询: 什么是大语言模型？\n",
                        "\n",
                        "初始检索结果（向量相似度排序）：\n",
                        "1. [距离: 0.7484] 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. [距离: 0.8587] 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "3. [距离: 0.8939] 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n",
                        "4. [距离: 1.0178] 检索增强生成（RAG）结合了信息检索和文本生成，可以提供更准确的答案。...\n",
                        "5. [距离: 1.0254] 北京是中国的首都，拥有悠久的历史和丰富的文化遗产。...\n"
                    ]
                }
            ],
            "source": [
                "query = \"什么是大语言模型？\"\n",
                "\n",
                "# 执行向量检索，获取前5个结果\n",
                "results = collection.query(\n",
                "    query_texts=[query],\n",
                "    n_results=5\n",
                ")\n",
                "\n",
                "print(f\"查询: {query}\\n\")\n",
                "print(\"初始检索结果（向量相似度排序）：\")\n",
                "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
                "    print(f\"{i+1}. [距离: {distance:.4f}] {doc[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b9ef59a",
            "metadata": {},
            "source": [
                "### 1.4 跨编码器重排（Cross-Encoder Reranking）\n",
                "使用 Cross-Encoder 模型对初步检索结果进行重新打分和排序。Cross-Encoder 同时处理查询和文档，可以捕捉更深层的语义关系。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "05c88883",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "重排后的结果（Cross-Encoder 排序）：\n",
                        "1. [重排分数: 7.5419] 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. [重排分数: 6.9363] 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n",
                        "3. [重排分数: 6.5408] 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "4. [重排分数: 6.3104] 检索增强生成（RAG）结合了信息检索和文本生成，可以提供更准确的答案。...\n",
                        "5. [重排分数: 1.6864] 北京是中国的首都，拥有悠久的历史和丰富的文化遗产。...\n"
                    ]
                }
            ],
            "source": [
                "from sentence_transformers import CrossEncoder\n",
                "import numpy as np\n",
                "\n",
                "# 加载 Cross-Encoder 模型\n",
                "rerank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
                "\n",
                "# 获取初步检索的文档\n",
                "candidate_docs = results['documents'][0]\n",
                "\n",
                "# 构建查询-文档对\n",
                "pairs = [[query, doc] for doc in candidate_docs]\n",
                "\n",
                "# 使用 Cross-Encoder 重新打分\n",
                "rerank_scores = rerank_model.predict(pairs)\n",
                "\n",
                "# 按重排分数排序\n",
                "reranked_indices = np.argsort(rerank_scores)[::-1]\n",
                "\n",
                "print(\"\\n重排后的结果（Cross-Encoder 排序）：\")\n",
                "for rank, idx in enumerate(reranked_indices):\n",
                "    print(f\"{rank+1}. [重排分数: {rerank_scores[idx]:.4f}] {candidate_docs[idx][:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9350a99d",
            "metadata": {},
            "source": [
                "### 1.5 基于多样性的重排\n",
                "除了相关性，我们还可以考虑结果的多样性，避免返回过于相似的文档。使用 MMR（Maximal Marginal Relevance）算法。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "490a4e3b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "MMR 重排结果（平衡相关性和多样性）：\n",
                        "1. 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "3. 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "def maximal_marginal_relevance(query_embedding, doc_embeddings, doc_texts, lambda_param=0.5, k=3):\n",
                "    \"\"\"\n",
                "    MMR 算法实现\n",
                "    lambda_param: 控制相关性和多样性的权重 (0-1)\n",
                "                 1 = 只考虑相关性，0 = 只考虑多样性\n",
                "    \"\"\"\n",
                "    selected_indices = []\n",
                "    remaining_indices = list(range(len(doc_embeddings)))\n",
                "    \n",
                "    # 计算所有文档与查询的相似度\n",
                "    query_similarities = cosine_similarity(\n",
                "        [query_embedding], \n",
                "        doc_embeddings\n",
                "    )[0]\n",
                "    \n",
                "    while len(selected_indices) < k and remaining_indices:\n",
                "        mmr_scores = []\n",
                "        \n",
                "        for idx in remaining_indices:\n",
                "            # 相关性分数\n",
                "            relevance = query_similarities[idx]\n",
                "            \n",
                "            # 多样性分数（与已选文档的最大相似度）\n",
                "            if selected_indices:\n",
                "                selected_embeddings = [doc_embeddings[i] for i in selected_indices]\n",
                "                diversity = max(cosine_similarity(\n",
                "                    [doc_embeddings[idx]], \n",
                "                    selected_embeddings\n",
                "                )[0])\n",
                "            else:\n",
                "                diversity = 0\n",
                "            \n",
                "            # MMR 分数\n",
                "            mmr_score = lambda_param * relevance - (1 - lambda_param) * diversity\n",
                "            mmr_scores.append(mmr_score)\n",
                "        \n",
                "        # 选择 MMR 分数最高的文档\n",
                "        best_idx_pos = np.argmax(mmr_scores)\n",
                "        best_idx = remaining_indices[best_idx_pos]\n",
                "        selected_indices.append(best_idx)\n",
                "        remaining_indices.remove(best_idx)\n",
                "    \n",
                "    return selected_indices\n",
                "\n",
                "# 获取查询和文档的嵌入\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "query_emb = model.encode(query)\n",
                "doc_embs = model.encode(candidate_docs)\n",
                "\n",
                "# 应用 MMR\n",
                "mmr_indices = maximal_marginal_relevance(\n",
                "    query_emb, \n",
                "    doc_embs, \n",
                "    candidate_docs, \n",
                "    lambda_param=0.7,\n",
                "    k=3\n",
                ")\n",
                "\n",
                "print(\"\\nMMR 重排结果（平衡相关性和多样性）：\")\n",
                "for rank, idx in enumerate(mmr_indices):\n",
                "    print(f\"{rank+1}. {candidate_docs[idx][:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "491bcd00",
            "metadata": {},
            "source": [
                "## 任务二：压缩技术（Compression）\n",
                "\n",
                "### 2.1 基于相关性的过滤\n",
                "设置相似度阈值，过滤掉不够相关的文档。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f92a1b2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "原始结果数量: 5\n",
                        "过滤后结果数量: 1\n",
                        "\n",
                        "过滤后的文档：\n",
                        "1. [相似度: 0.2516] 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n"
                    ]
                }
            ],
            "source": [
                "def filter_by_relevance(documents, scores, threshold=0.3):\n",
                "    \"\"\"\n",
                "    根据相关性分数过滤文档\n",
                "    \"\"\"\n",
                "    filtered = []\n",
                "    for doc, score in zip(documents, scores):\n",
                "        if score >= threshold:\n",
                "            filtered.append((doc, score))\n",
                "    return filtered\n",
                "\n",
                "# 应用相关性过滤\n",
                "# 注意：distance 越小越相似，这里转换为相似度分数\n",
                "similarity_scores = [1 - d for d in results['distances'][0]]\n",
                "filtered_results = filter_by_relevance(\n",
                "    candidate_docs, \n",
                "    similarity_scores, \n",
                "    threshold=0.2\n",
                ")\n",
                "\n",
                "print(f\"原始结果数量: {len(candidate_docs)}\")\n",
                "print(f\"过滤后结果数量: {len(filtered_results)}\")\n",
                "print(\"\\n过滤后的文档：\")\n",
                "for i, (doc, score) in enumerate(filtered_results):\n",
                "    print(f\"{i+1}. [相似度: {score:.4f}] {doc[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a3c7d4e",
            "metadata": {},
            "source": [
                "### 2.2 上下文压缩（Contextual Compression）\n",
                "使用 LangChain 的上下文压缩器，从检索到的文档中提取与查询最相关的部分。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b8c9d5f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/y5/mq71659d69b2_wlmhhvy7lw00000gn/T/ipykernel_25942/253050070.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
                        "/opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "压缩前文档数量: 5\n",
                        "压缩后文档数量: 3\n",
                        "\n",
                        "压缩后的文档：\n",
                        "1. 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "3. 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n"
                    ]
                }
            ],
            "source": [
                "from langchain.retrievers import ContextualCompressionRetriever\n",
                "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain.schema import Document\n",
                "\n",
                "# 创建文档对象\n",
                "docs = [Document(page_content=doc) for doc in candidate_docs]\n",
                "\n",
                "# 创建嵌入过滤器\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
                "embeddings_filter = EmbeddingsFilter(\n",
                "    embeddings=embeddings,\n",
                "    similarity_threshold=0.5\n",
                ")\n",
                "\n",
                "# 手动压缩文档\n",
                "compressed_docs = embeddings_filter.compress_documents(docs, query)\n",
                "\n",
                "print(f\"\\n压缩前文档数量: {len(docs)}\")\n",
                "print(f\"压缩后文档数量: {len(compressed_docs)}\")\n",
                "print(\"\\n压缩后的文档：\")\n",
                "for i, doc in enumerate(compressed_docs):\n",
                "    print(f\"{i+1}. {doc.page_content[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c6d8e9f",
            "metadata": {},
            "source": [
                "### 2.3 文本摘要压缩\n",
                "对于长文档，可以使用摘要技术提取关键信息，减少 token 消耗。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a7b8c0d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "文本压缩示例：\n",
                        "\n",
                        "原文 1: 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。\n",
                        "压缩 1: 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥...\n",
                        "压缩率: 78.6%\n",
                        "\n",
                        "原文 2: 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。\n",
                        "压缩 2: 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性...\n",
                        "压缩率: 106.5%\n",
                        "\n",
                        "原文 3: 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。\n",
                        "压缩 3: 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RA...\n",
                        "压缩率: 78.6%\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "def simple_compression(text, max_length=50):\n",
                "    \"\"\"\n",
                "    简单的文本压缩：截取前 N 个字符\n",
                "    实际应用中可以使用 LLM 进行智能摘要\n",
                "    \"\"\"\n",
                "    if len(text) <= max_length:\n",
                "        return text\n",
                "    return text[:max_length] + \"...\"\n",
                "\n",
                "print(\"文本压缩示例：\\n\")\n",
                "for i, doc in enumerate(candidate_docs[:3]):\n",
                "    compressed = simple_compression(doc, max_length=30)\n",
                "    print(f\"原文 {i+1}: {doc}\")\n",
                "    print(f\"压缩 {i+1}: {compressed}\")\n",
                "    print(f\"压缩率: {len(compressed)/len(doc)*100:.1f}%\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e4f5a6b",
            "metadata": {},
            "source": [
                "## 任务三：校正技术（Correction）\n",
                "\n",
                "### 3.1 查询扩展（Query Expansion）\n",
                "通过添加同义词或相关词来扩展查询，提高召回率。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d0e1f2c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "原始查询: 什么是大语言模型？\n",
                        "扩展查询: 什么是大语言模型？ LLM 大模型 语言模型\n",
                        "\n",
                        "扩展查询的检索结果：\n",
                        "1. 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "3. 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n"
                    ]
                }
            ],
            "source": [
                "def expand_query(query, synonyms_dict):\n",
                "    \"\"\"\n",
                "    简单的查询扩展\n",
                "    \"\"\"\n",
                "    expanded_terms = [query]\n",
                "    \n",
                "    for word, synonyms in synonyms_dict.items():\n",
                "        if word in query:\n",
                "            expanded_terms.extend(synonyms)\n",
                "    \n",
                "    return \" \".join(expanded_terms)\n",
                "\n",
                "# 定义同义词字典\n",
                "synonyms = {\n",
                "    \"大语言模型\": [\"LLM\", \"大模型\", \"语言模型\"],\n",
                "    \"检索\": [\"搜索\", \"查询\", \"查找\"],\n",
                "}\n",
                "\n",
                "original_query = \"什么是大语言模型？\"\n",
                "expanded_query = expand_query(original_query, synonyms)\n",
                "\n",
                "print(f\"原始查询: {original_query}\")\n",
                "print(f\"扩展查询: {expanded_query}\")\n",
                "\n",
                "# 使用扩展查询进行检索\n",
                "expanded_results = collection.query(\n",
                "    query_texts=[expanded_query],\n",
                "    n_results=3\n",
                ")\n",
                "\n",
                "print(\"\\n扩展查询的检索结果：\")\n",
                "for i, doc in enumerate(expanded_results['documents'][0]):\n",
                "    print(f\"{i+1}. {doc[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b5c6d7e",
            "metadata": {},
            "source": [
                "### 3.2 查询重写（Query Rewriting）\n",
                "将用户的自然语言查询转换为更适合检索的形式。在实际应用中，可以使用 LLM 进行智能重写。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c7d8e9f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "查询重写示例：\n",
                        "\n",
                        "原始: 什么是大语言模型？\n",
                        "重写: 大语言模型\n",
                        "\n",
                        "原始: 如何使用向量数据库？\n",
                        "重写: 使用向量数据库\n",
                        "\n",
                        "原始: 为什么 RAG 系统很重要？\n",
                        "重写: RAG 系统很重要\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "def rewrite_query(query):\n",
                "    \"\"\"\n",
                "    简单的查询重写规则\n",
                "    实际应用中应使用 LLM 进行智能重写\n",
                "    \"\"\"\n",
                "    # 移除疑问词\n",
                "    question_words = [\"什么是\", \"如何\", \"怎么\", \"为什么\", \"？\", \"?\"]\n",
                "    rewritten = query\n",
                "    \n",
                "    for word in question_words:\n",
                "        rewritten = rewritten.replace(word, \"\")\n",
                "    \n",
                "    # 去除多余空格\n",
                "    rewritten = \" \".join(rewritten.split())\n",
                "    \n",
                "    return rewritten\n",
                "\n",
                "# 测试查询重写\n",
                "test_queries = [\n",
                "    \"什么是大语言模型？\",\n",
                "    \"如何使用向量数据库？\",\n",
                "    \"为什么 RAG 系统很重要？\"\n",
                "]\n",
                "\n",
                "print(\"查询重写示例：\\n\")\n",
                "for q in test_queries:\n",
                "    rewritten = rewrite_query(q)\n",
                "    print(f\"原始: {q}\")\n",
                "    print(f\"重写: {rewritten}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f9e0a1b",
            "metadata": {},
            "source": [
                "### 3.3 混合检索策略（Hybrid Search）\n",
                "结合关键词检索（BM25）和向量检索，提高检索的准确性和召回率。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9518231",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
                        "Collecting jieba\n",
                        "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hBuilding wheels for collected packages: jieba\n",
                        "  Building wheel for jieba (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=3ea04589bcc7ac54f58fab7e5c6a8cde2a1d83b1c599360f2e56cbfd3569c501\n",
                        "  Stored in directory: /Users/lipucheng/Library/Caches/pip/wheels/32/98/ba/a37fcadb96c75c8f9366a3d17da29cdf8a745ffd38d0092e0d\n",
                        "Successfully built jieba\n",
                        "Installing collected packages: jieba\n",
                        "Successfully installed jieba-0.42.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install jieba"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5d6e7f8a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/Caskroom/miniconda/base/envs/rag-project01/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "  import pkg_resources\n",
                        "Building prefix dict from the default dictionary ...\n",
                        "Dumping model to file cache /var/folders/y5/mq71659d69b2_wlmhhvy7lw00000gn/T/jieba.cache\n",
                        "Loading model cost 0.311 seconds.\n",
                        "Prefix dict has been built successfully.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "混合检索结果（BM25 + 向量检索）：\n",
                        "\n",
                        "1. [混合分数: 1.0000] 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. [混合分数: 0.3612] 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "3. [混合分数: 0.2849] 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n"
                    ]
                }
            ],
            "source": [
                "from rank_bm25 import BM25Okapi\n",
                "import jieba\n",
                "\n",
                "def hybrid_search(query, documents, vector_weight=0.5, k=5):\n",
                "    \"\"\"\n",
                "    混合检索：结合 BM25 和向量检索\n",
                "    \"\"\"\n",
                "    # BM25 检索\n",
                "    tokenized_corpus = [list(jieba.cut(doc)) for doc in documents]\n",
                "    bm25 = BM25Okapi(tokenized_corpus)\n",
                "    tokenized_query = list(jieba.cut(query))\n",
                "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                "    \n",
                "    # 向量检索（使用之前的结果）\n",
                "    vector_results = collection.query(\n",
                "        query_texts=[query],\n",
                "        n_results=len(documents)\n",
                "    )\n",
                "    \n",
                "    # 归一化分数\n",
                "    bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
                "    \n",
                "    # 向量距离转换为相似度分数并归一化\n",
                "    vector_distances = results['distances'][0]\n",
                "    vector_scores = np.array([1 - d for d in vector_distances])\n",
                "    vector_scores_norm = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min() + 1e-10)\n",
                "    \n",
                "    # 混合分数（只对前5个文档计算）\n",
                "    hybrid_scores = []\n",
                "    for i in range(min(len(documents), len(vector_scores_norm))):\n",
                "        hybrid_score = vector_weight * vector_scores_norm[i] + (1 - vector_weight) * bm25_scores_norm[i]\n",
                "        hybrid_scores.append((i, hybrid_score))\n",
                "    \n",
                "    # 排序\n",
                "    hybrid_scores.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    return hybrid_scores[:k]\n",
                "\n",
                "# 执行混合检索\n",
                "hybrid_results = hybrid_search(\n",
                "    query=\"大语言模型\",\n",
                "    documents=candidate_docs,\n",
                "    vector_weight=0.6,\n",
                "    k=3\n",
                ")\n",
                "\n",
                "print(\"混合检索结果（BM25 + 向量检索）：\\n\")\n",
                "for rank, (idx, score) in enumerate(hybrid_results):\n",
                "    print(f\"{rank+1}. [混合分数: {score:.4f}] {candidate_docs[idx][:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9a0b1c2d",
            "metadata": {},
            "source": [
                "## 任务四：综合应用示例\n",
                "\n",
                "### 4.1 完整的检索后处理流程\n",
                "组合多种后处理技术，构建一个完整的检索增强管道。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6b7c8d9e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "高级检索管道流程\n",
                        "================================================================================\n",
                        "\n",
                        "步骤1 - 查询重写:\n",
                        "  原始查询: 什么是大语言模型？\n",
                        "  重写查询: 大语言模型\n",
                        "\n",
                        "步骤2 - 初始检索: 召回 10 个候选文档\n",
                        "\n",
                        "步骤3 - 重排序: 使用 Cross-Encoder 重新排序\n",
                        "\n",
                        "步骤4 - 相关性过滤: 保留 10 个高相关文档\n",
                        "\n",
                        "步骤5 - 多样性优化: 返回 3 个最终结果\n",
                        "\n",
                        "================================================================================\n",
                        "最终检索结果\n",
                        "================================================================================\n",
                        "\n",
                        "1. [分数: 8.1558]\n",
                        "   大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。\n",
                        "\n",
                        "2. [分数: 6.7359]\n",
                        "   向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。\n",
                        "\n",
                        "3. [分数: 6.2852]\n",
                        "   向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。\n"
                    ]
                }
            ],
            "source": [
                "def advanced_retrieval_pipeline(query, top_k=3):\n",
                "    \"\"\"\n",
                "    高级检索管道：\n",
                "    1. 查询重写\n",
                "    2. 初始检索（多召回）\n",
                "    3. 重排序（Cross-Encoder）\n",
                "    4. 压缩（相关性过滤）\n",
                "    5. 多样性优化（MMR）\n",
                "    \"\"\"\n",
                "    print(\"=\"*80)\n",
                "    print(\"高级检索管道流程\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # 步骤1: 查询重写\n",
                "    rewritten_query = rewrite_query(query)\n",
                "    print(f\"\\n步骤1 - 查询重写:\")\n",
                "    print(f\"  原始查询: {query}\")\n",
                "    print(f\"  重写查询: {rewritten_query}\")\n",
                "    \n",
                "    # 步骤2: 初始检索（召回更多候选）\n",
                "    initial_results = collection.query(\n",
                "        query_texts=[rewritten_query],\n",
                "        n_results=10\n",
                "    )\n",
                "    candidates = initial_results['documents'][0]\n",
                "    print(f\"\\n步骤2 - 初始检索: 召回 {len(candidates)} 个候选文档\")\n",
                "    \n",
                "    # 步骤3: 重排序\n",
                "    pairs = [[rewritten_query, doc] for doc in candidates]\n",
                "    rerank_scores = rerank_model.predict(pairs)\n",
                "    reranked_indices = np.argsort(rerank_scores)[::-1]\n",
                "    print(f\"\\n步骤3 - 重排序: 使用 Cross-Encoder 重新排序\")\n",
                "    \n",
                "    # 步骤4: 相关性过滤\n",
                "    threshold = 0.0  # Cross-Encoder 分数阈值\n",
                "    filtered_candidates = []\n",
                "    filtered_scores = []\n",
                "    for idx in reranked_indices:\n",
                "        if rerank_scores[idx] >= threshold:\n",
                "            filtered_candidates.append(candidates[idx])\n",
                "            filtered_scores.append(rerank_scores[idx])\n",
                "    \n",
                "    print(f\"\\n步骤4 - 相关性过滤: 保留 {len(filtered_candidates)} 个高相关文档\")\n",
                "    \n",
                "    # 步骤5: 多样性优化（MMR）\n",
                "    if len(filtered_candidates) > top_k:\n",
                "        doc_embs = model.encode(filtered_candidates)\n",
                "        query_emb = model.encode(rewritten_query)\n",
                "        mmr_indices = maximal_marginal_relevance(\n",
                "            query_emb,\n",
                "            doc_embs,\n",
                "            filtered_candidates,\n",
                "            lambda_param=0.7,\n",
                "            k=top_k\n",
                "        )\n",
                "        final_docs = [filtered_candidates[i] for i in mmr_indices]\n",
                "        final_scores = [filtered_scores[i] for i in mmr_indices]\n",
                "    else:\n",
                "        final_docs = filtered_candidates[:top_k]\n",
                "        final_scores = filtered_scores[:top_k]\n",
                "    \n",
                "    print(f\"\\n步骤5 - 多样性优化: 返回 {len(final_docs)} 个最终结果\")\n",
                "    \n",
                "    # 输出最终结果\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"最终检索结果\")\n",
                "    print(\"=\"*80)\n",
                "    for i, (doc, score) in enumerate(zip(final_docs, final_scores)):\n",
                "        print(f\"\\n{i+1}. [分数: {score:.4f}]\")\n",
                "        print(f\"   {doc}\")\n",
                "    \n",
                "    return final_docs\n",
                "\n",
                "# 测试完整管道\n",
                "test_query = \"什么是大语言模型？\"\n",
                "final_results = advanced_retrieval_pipeline(test_query, top_k=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c8d9e0f",
            "metadata": {},
            "source": [
                "### 4.2 对比不同方法的效果\n",
                "比较基础检索、重排序和完整管道的检索效果。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d9e0f1a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "查询: 大语言模型的应用\n",
                        "\n",
                        "================================================================================\n",
                        "\n",
                        "方法1: 基础向量检索\n",
                        "--------------------------------------------------------------------------------\n",
                        "1. 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n",
                        "3. 向量嵌入可以将文本转换为数值向量，使计算机能够理解语义相似性。...\n",
                        "\n",
                        "方法2: 向量检索 + Cross-Encoder 重排\n",
                        "--------------------------------------------------------------------------------\n",
                        "1. [分数: 8.2927] 大语言模型（LLM）是一种基于深度学习的自然语言处理模型，拥有数十亿甚至数千亿参数。...\n",
                        "2. [分数: 7.5426] 向量数据库是专门用于存储和检索高维向量的数据库系统，在 RAG 系统中扮演重要角色。...\n",
                        "3. [分数: 7.2078] 检索增强生成（RAG）结合了信息检索和文本生成，可以提供更准确的答案。...\n",
                        "\n",
                        "方法3: 完整后处理管道\n",
                        "--------------------------------------------------------------------------------\n",
                        "(参见上面的完整管道输出)\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "def compare_retrieval_methods(query):\n",
                "    \"\"\"\n",
                "    对比不同检索方法\n",
                "    \"\"\"\n",
                "    print(f\"查询: {query}\")\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    \n",
                "    # 方法1: 基础向量检索\n",
                "    basic_results = collection.query(\n",
                "        query_texts=[query],\n",
                "        n_results=3\n",
                "    )\n",
                "    print(\"\\n方法1: 基础向量检索\")\n",
                "    print(\"-\" * 80)\n",
                "    for i, doc in enumerate(basic_results['documents'][0]):\n",
                "        print(f\"{i+1}. {doc[:70]}...\")\n",
                "    \n",
                "    # 方法2: 向量检索 + Cross-Encoder 重排\n",
                "    candidates = collection.query(query_texts=[query], n_results=5)['documents'][0]\n",
                "    pairs = [[query, doc] for doc in candidates]\n",
                "    scores = rerank_model.predict(pairs)\n",
                "    reranked_idx = np.argsort(scores)[::-1][:3]\n",
                "    \n",
                "    print(\"\\n方法2: 向量检索 + Cross-Encoder 重排\")\n",
                "    print(\"-\" * 80)\n",
                "    for i, idx in enumerate(reranked_idx):\n",
                "        print(f\"{i+1}. [分数: {scores[idx]:.4f}] {candidates[idx][:60]}...\")\n",
                "    \n",
                "    # 方法3: 完整管道（已在上面运行）\n",
                "    print(\"\\n方法3: 完整后处理管道\")\n",
                "    print(\"-\" * 80)\n",
                "    print(\"(参见上面的完整管道输出)\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "\n",
                "# 运行对比\n",
                "compare_retrieval_methods(\"大语言模型的应用\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a2b3c4d",
            "metadata": {},
            "source": [
                "## 作业2：Sakila Text2SQL 评估体系\n",
                "\n",
                "### 2.1 任务背景\n",
                "Text2SQL 是将自然语言问题转换为 SQL 查询的任务。本作业将构建一个评估体系，用于评估 Text2SQL 系统的性能。\n",
                "\n",
                "我们将使用 Sakila 数据库的问题-SQL对作为评估数据集，实现基于向量检索的 Few-Shot 示例选择，并评估生成的 SQL 质量。"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b3c4d5e",
            "metadata": {},
            "source": [
                "### 2.2 加载评估数据集\n",
                "首先加载 Sakila Text2SQL 数据集。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "3c4d5e6f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "数据集大小: 36 条\n",
                        "\n",
                        "数据集概览:\n",
                        "                                            question  \\\n",
                        "0          List all actors with their IDs and names.   \n",
                        "1                  Add a new actor named 'John Doe'.   \n",
                        "2  Update the last name of actor with ID 1 to 'Sm...   \n",
                        "3                        Delete the actor with ID 2.   \n",
                        "4             Show all films and their descriptions.   \n",
                        "\n",
                        "                                                 sql  \n",
                        "0  SELECT actor_id, first_name, last_name FROM ac...  \n",
                        "1  INSERT INTO actor (first_name, last_name) VALU...  \n",
                        "2  UPDATE actor SET last_name = 'Smith' WHERE act...  \n",
                        "3              DELETE FROM actor WHERE actor_id = 2;  \n",
                        "4      SELECT film_id, title, description FROM film;  \n",
                        "\n",
                        "数据集统计:\n",
                        "- 平均问题长度: 39.5 字符\n",
                        "- 平均 SQL 长度: 59.7 字符\n",
                        "\n",
                        "SQL 类型分布:\n",
                        "  SELECT: 9 (25.0%)\n",
                        "  INSERT: 9 (25.0%)\n",
                        "  UPDATE: 9 (25.0%)\n",
                        "  DELETE: 9 (25.0%)\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "# 加载数据集\n",
                "data_path = Path('sample_data/q2sql_pairs.json')\n",
                "with open(data_path, 'r', encoding='utf-8') as f:\n",
                "    text2sql_data = json.load(f)\n",
                "\n",
                "# 转换为 DataFrame 便于分析\n",
                "df = pd.DataFrame(text2sql_data)\n",
                "\n",
                "print(f\"数据集大小: {len(df)} 条\")\n",
                "print(f\"\\n数据集概览:\")\n",
                "print(df.head())\n",
                "\n",
                "# 分析数据集\n",
                "print(\"\\n数据集统计:\")\n",
                "print(f\"- 平均问题长度: {df['question'].str.len().mean():.1f} 字符\")\n",
                "print(f\"- 平均 SQL 长度: {df['sql'].str.len().mean():.1f} 字符\")\n",
                "\n",
                "# 分析 SQL 类型分布\n",
                "sql_types = df['sql'].str.extract(r'^(SELECT|INSERT|UPDATE|DELETE)')[0].value_counts()\n",
                "print(\"\\nSQL 类型分布:\")\n",
                "for sql_type, count in sql_types.items():\n",
                "    print(f\"  {sql_type}: {count} ({count/len(df)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d5e6f7g",
            "metadata": {},
            "source": [
                "### 2.3 构建向量检索系统\n",
                "使用向量检索为给定问题找到最相似的示例，用于 Few-Shot 学习。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5e6f7g8h",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "已添加 36 个 Text2SQL 示例到向量库\n"
                    ]
                }
            ],
            "source": [
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "# 创建 Text2SQL 示例库\n",
                "text2sql_client = chromadb.Client()\n",
                "text2sql_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                "    model_name=\"all-MiniLM-L6-v2\"\n",
                ")\n",
                "\n",
                "# 创建 collection\n",
                "text2sql_collection = text2sql_client.create_collection(\n",
                "    name=\"text2sql_examples\",\n",
                "    embedding_function=text2sql_ef\n",
                ")\n",
                "\n",
                "# 添加所有示例（将问题和SQL组合作为文档）\n",
                "documents = []\n",
                "metadatas = []\n",
                "ids = []\n",
                "\n",
                "for idx, row in df.iterrows():\n",
                "    # 使用问题作为检索的主要内容\n",
                "    doc = row['question']\n",
                "    documents.append(doc)\n",
                "    metadatas.append({\n",
                "        'question': row['question'],\n",
                "        'sql': row['sql']\n",
                "    })\n",
                "    ids.append(f\"example_{idx}\")\n",
                "\n",
                "text2sql_collection.add(\n",
                "    documents=documents,\n",
                "    metadatas=metadatas,\n",
                "    ids=ids\n",
                ")\n",
                "\n",
                "print(f\"已添加 {len(documents)} 个 Text2SQL 示例到向量库\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f7g8h9i",
            "metadata": {},
            "source": [
                "### 2.4 实现示例检索功能\n",
                "为新问题检索最相关的 Few-Shot 示例。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "7g8h9i0j",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "问题: Find all actors whose first name is 'John'.\n",
                        "\n",
                        "检索到的相似示例:\n",
                        "\n",
                        "1. [距离: 0.5495]\n",
                        "   问题: List all actors with their IDs and names.\n",
                        "   SQL: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "\n",
                        "2. [距离: 0.7341]\n",
                        "   问题: Add a new actor named 'John Doe'.\n",
                        "   SQL: INSERT INTO actor (first_name, last_name) VALUES ('John', 'Doe');\n",
                        "\n",
                        "3. [距离: 0.9341]\n",
                        "   问题: Update the last name of actor with ID 1 to 'Smith'.\n",
                        "   SQL: UPDATE actor SET last_name = 'Smith' WHERE actor_id = 1;\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "def retrieve_examples(question, n_examples=3):\n",
                "    \"\"\"\n",
                "    为给定问题检索最相关的示例\n",
                "    \"\"\"\n",
                "    results = text2sql_collection.query(\n",
                "        query_texts=[question],\n",
                "        n_results=n_examples\n",
                "    )\n",
                "    \n",
                "    examples = []\n",
                "    for i, metadata in enumerate(results['metadatas'][0]):\n",
                "        examples.append({\n",
                "            'question': metadata['question'],\n",
                "            'sql': metadata['sql'],\n",
                "            'distance': results['distances'][0][i]\n",
                "        })\n",
                "    \n",
                "    return examples\n",
                "\n",
                "# 测试示例检索\n",
                "test_question = \"Find all actors whose first name is 'John'.\"\n",
                "retrieved_examples = retrieve_examples(test_question, n_examples=3)\n",
                "\n",
                "print(f\"问题: {test_question}\\n\")\n",
                "print(\"检索到的相似示例:\\n\")\n",
                "for i, ex in enumerate(retrieved_examples):\n",
                "    print(f\"{i+1}. [距离: {ex['distance']:.4f}]\")\n",
                "    print(f\"   问题: {ex['question']}\")\n",
                "    print(f\"   SQL: {ex['sql']}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8h9i0j1k",
            "metadata": {},
            "source": [
                "### 2.5 构建 Few-Shot Prompt\n",
                "基于检索到的示例构建 Few-Shot 提示词。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "9i0j1k2l",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "构建的 Few-Shot Prompt:\n",
                        "\n",
                        "================================================================================\n",
                        "You are an expert SQL query generator for the Sakila database.\n",
                        "Given a natural language question, generate the corresponding SQL query.\n",
                        "\n",
                        "Here are some examples:\n",
                        "\n",
                        "Example 1:\n",
                        "Question: Show all films and their descriptions.\n",
                        "SQL: SELECT film_id, title, description FROM film;\n",
                        "\n",
                        "Example 2:\n",
                        "Question: Show inventory items for film ID 5.\n",
                        "SQL: SELECT inventory_id, film_id, store_id FROM inventory WHERE film_id = 5;\n",
                        "\n",
                        "Example 3:\n",
                        "Question: List all actors with their IDs and names.\n",
                        "SQL: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "\n",
                        "Now generate the SQL query for this question:\n",
                        "Question: Show all films released in 2006.\n",
                        "SQL:\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "def build_few_shot_prompt(question, examples, n_shots=3):\n",
                "    \"\"\"\n",
                "    构建 Few-Shot 提示词\n",
                "    \"\"\"\n",
                "    prompt = \"\"\"You are an expert SQL query generator for the Sakila database.\n",
                "Given a natural language question, generate the corresponding SQL query.\n",
                "\n",
                "Here are some examples:\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    # 添加示例\n",
                "    for i, ex in enumerate(examples[:n_shots]):\n",
                "        prompt += f\"Example {i+1}:\\n\"\n",
                "        prompt += f\"Question: {ex['question']}\\n\"\n",
                "        prompt += f\"SQL: {ex['sql']}\\n\\n\"\n",
                "    \n",
                "    # 添加新问题\n",
                "    prompt += f\"Now generate the SQL query for this question:\\n\"\n",
                "    prompt += f\"Question: {question}\\n\"\n",
                "    prompt += f\"SQL:\"\n",
                "    \n",
                "    return prompt\n",
                "\n",
                "# 测试 Few-Shot Prompt 构建\n",
                "test_question = \"Show all films released in 2006.\"\n",
                "examples = retrieve_examples(test_question, n_examples=3)\n",
                "prompt = build_few_shot_prompt(test_question, examples, n_shots=3)\n",
                "\n",
                "print(\"构建的 Few-Shot Prompt:\\n\")\n",
                "print(\"=\"*80)\n",
                "print(prompt)\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0j1k2l3m",
            "metadata": {},
            "source": [
                "### 2.6 评估指标实现\n",
                "\n",
                "实现常用的 Text2SQL 评估指标：\n",
                "1. **精确匹配（Exact Match）**: SQL 语句完全相同\n",
                "2. **标准化匹配（Normalized Match）**: 忽略大小写和空格\n",
                "3. **关键词匹配（Keyword Match）**: 检查关键 SQL 关键词是否匹配"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "1k2l3m4n",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "评估指标测试:\n",
                        "\n",
                        "测试 1: 完全匹配\n",
                        "  预测: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "  真实: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "  结果:\n",
                        "    - 精确匹配: True\n",
                        "    - 标准化匹配: True\n",
                        "    - 关键词分数: 1.00\n",
                        "\n",
                        "测试 2: 大小写不同，无分号\n",
                        "  预测: select actor_id, first_name, last_name from actor\n",
                        "  真实: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "  结果:\n",
                        "    - 精确匹配: False\n",
                        "    - 标准化匹配: True\n",
                        "    - 关键词分数: 1.00\n",
                        "\n",
                        "测试 3: 列名不同\n",
                        "  预测: SELECT * FROM actor;\n",
                        "  真实: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "  结果:\n",
                        "    - 精确匹配: False\n",
                        "    - 标准化匹配: False\n",
                        "    - 关键词分数: 1.00\n",
                        "\n",
                        "测试 4: 表名不同\n",
                        "  预测: SELECT * FROM film;\n",
                        "  真实: SELECT actor_id, first_name, last_name FROM actor;\n",
                        "  结果:\n",
                        "    - 精确匹配: False\n",
                        "    - 标准化匹配: False\n",
                        "    - 关键词分数: 0.50\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from typing import Dict, List\n",
                "\n",
                "def normalize_sql(sql: str) -> str:\n",
                "    \"\"\"\n",
                "    标准化 SQL 查询\n",
                "    \"\"\"\n",
                "    # 转为小写\n",
                "    sql = sql.lower()\n",
                "    # 移除多余空格\n",
                "    sql = ' '.join(sql.split())\n",
                "    # 移除末尾分号\n",
                "    sql = sql.rstrip(';')\n",
                "    return sql\n",
                "\n",
                "def exact_match(predicted: str, ground_truth: str) -> bool:\n",
                "    \"\"\"\n",
                "    精确匹配评估\n",
                "    \"\"\"\n",
                "    return predicted.strip() == ground_truth.strip()\n",
                "\n",
                "def normalized_match(predicted: str, ground_truth: str) -> bool:\n",
                "    \"\"\"\n",
                "    标准化匹配评估\n",
                "    \"\"\"\n",
                "    return normalize_sql(predicted) == normalize_sql(ground_truth)\n",
                "\n",
                "def extract_sql_keywords(sql: str) -> set:\n",
                "    \"\"\"\n",
                "    提取 SQL 中的关键词和表名\n",
                "    \"\"\"\n",
                "    sql_normalized = normalize_sql(sql)\n",
                "    # 提取主要关键词\n",
                "    keywords = set()\n",
                "    \n",
                "    # SQL 操作类型\n",
                "    for op in ['select', 'insert', 'update', 'delete', 'from', 'where', 'join', 'order by', 'group by']:\n",
                "        if op in sql_normalized:\n",
                "            keywords.add(op)\n",
                "    \n",
                "    # 提取表名 (简单的启发式方法)\n",
                "    # FROM 后面的词通常是表名\n",
                "    from_pattern = r'from\\s+(\\w+)'\n",
                "    from_matches = re.findall(from_pattern, sql_normalized)\n",
                "    keywords.update(from_matches)\n",
                "    \n",
                "    # UPDATE 后面的词通常是表名\n",
                "    update_pattern = r'update\\s+(\\w+)'\n",
                "    update_matches = re.findall(update_pattern, sql_normalized)\n",
                "    keywords.update(update_matches)\n",
                "    \n",
                " # INSERT INTO 后面的词通常是表名\n",
                "    insert_pattern = r'insert\\s+into\\s+(\\w+)'\n",
                "    insert_matches = re.findall(insert_pattern, sql_normalized)\n",
                "    keywords.update(insert_matches)\n",
                "    \n",
                "    return keywords\n",
                "\n",
                "def keyword_match_score(predicted: str, ground_truth: str) -> float:\n",
                "    \"\"\"\n",
                "    关键词匹配分数（Jaccard 相似度）\n",
                "    \"\"\"\n",
                "    pred_keywords = extract_sql_keywords(predicted)\n",
                "    gt_keywords = extract_sql_keywords(ground_truth)\n",
                "    \n",
                "    if not pred_keywords and not gt_keywords:\n",
                "        return 1.0\n",
                "    if not pred_keywords or not gt_keywords:\n",
                "        return 0.0\n",
                "    \n",
                "    intersection = pred_keywords & gt_keywords\n",
                "    union = pred_keywords | gt_keywords\n",
                "    \n",
                "    return len(intersection) / len(union)\n",
                "\n",
                "def evaluate_sql(predicted: str, ground_truth: str) -> Dict[str, any]:\n",
                "    \"\"\"\n",
                "    综合评估一个 SQL 预测\n",
                "    \"\"\"\n",
                "    return {\n",
                "        'exact_match': exact_match(predicted, ground_truth),\n",
                "        'normalized_match': normalized_match(predicted, ground_truth),\n",
                "        'keyword_score': keyword_match_score(predicted, ground_truth)\n",
                "    }\n",
                "\n",
                "# 测试评估指标\n",
                "print(\"评估指标测试:\\n\")\n",
                "\n",
                "test_cases = [\n",
                "    {\n",
                "        'predicted': 'SELECT actor_id, first_name, last_name FROM actor;',\n",
                "        'ground_truth': 'SELECT actor_id, first_name, last_name FROM actor;',\n",
                "        'description': '完全匹配'\n",
                "    },\n",
                "    {\n",
                "        'predicted': 'select actor_id, first_name, last_name from actor',\n",
                "        'ground_truth': 'SELECT actor_id, first_name, last_name FROM actor;',\n",
                "        'description': '大小写不同，无分号'\n",
                "    },\n",
                "    {\n",
                "        'predicted': 'SELECT * FROM actor;',\n",
                "        'ground_truth': 'SELECT actor_id, first_name, last_name FROM actor;',\n",
                "        'description': '列名不同'\n",
                "    },\n",
                "    {\n",
                "        'predicted': 'SELECT * FROM film;',\n",
                "        'ground_truth': 'SELECT actor_id, first_name, last_name FROM actor;',\n",
                "        'description': '表名不同'\n",
                "    }\n",
                "]\n",
                "\n",
                "for i, test in enumerate(test_cases):\n",
                "    print(f\"测试 {i+1}: {test['description']}\")\n",
                "    print(f\"  预测: {test['predicted']}\")\n",
                "    print(f\"  真实: {test['ground_truth']}\")\n",
                "    \n",
                "    results = evaluate_sql(test['predicted'], test['ground_truth'])\n",
                "    print(f\"  结果:\")\n",
                "    print(f\"    - 精确匹配: {results['exact_match']}\")\n",
                "    print(f\"    - 标准化匹配: {results['normalized_match']}\")\n",
                "    print(f\"    - 关键词分数: {results['keyword_score']:.2f}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2l3m4n5o",
            "metadata": {},
            "source": [
                "### 2.7 完整评估流程\n",
                "\n",
                "实现完整的 Text2SQL 评估流程，包括：\n",
                "1. 数据集划分（训练集用于检索示例，测试集用于评估）\n",
                "2. 对测试集的每个问题检索示例\n",
                "3. 模拟生成 SQL（这里使用检索到的最相似示例的 SQL 作为预测）\n",
                "4. 计算评估指标"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "3m4n5o6p",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "训练集大小: 28\n",
                        "测试集大小: 8\n",
                        "\n",
                        "已添加 28 个训练示例到评估向量库\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# 划分数据集（80% 训练，20% 测试）\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"训练集大小: {len(train_df)}\")\n",
                "print(f\"测试集大小: {len(test_df)}\")\n",
                "\n",
                "# 重新构建向量库（只使用训练集）\n",
                "eval_client = chromadb.Client()\n",
                "eval_collection = eval_client.create_collection(\n",
                "    name=\"text2sql_train\",\n",
                "    embedding_function=text2sql_ef\n",
                ")\n",
                "\n",
                "# 添加训练集示例\n",
                "train_docs = []\n",
                "train_metas = []\n",
                "train_ids = []\n",
                "\n",
                "for idx, row in train_df.iterrows():\n",
                "    train_docs.append(row['question'])\n",
                "    train_metas.append({\n",
                "        'question': row['question'],\n",
                "        'sql': row['sql']\n",
                "    })\n",
                "    train_ids.append(f\"train_{idx}\")\n",
                "\n",
                "eval_collection.add(\n",
                "    documents=train_docs,\n",
                "    metadatas=train_metas,\n",
                "    ids=train_ids\n",
                ")\n",
                "\n",
                "print(f\"\\n已添加 {len(train_docs)} 个训练示例到评估向量库\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "4n5o6p7q",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "正在评估 Text2SQL 系统...\n",
                        "\n",
                        "================================================================================\n",
                        "评估结果汇总\n",
                        "================================================================================\n",
                        "测试样本数: 8\n",
                        "\n",
                        "准确率指标:\n",
                        "  - 精确匹配率: 0.00%\n",
                        "  - 标准化匹配率: 0.00%\n",
                        "  - 平均关键词分数: 0.4333\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "def evaluate_text2sql_system(test_data, collection, use_top_k=1):\n",
                "    \"\"\"\n",
                "    评估 Text2SQL 系统\n",
                "    use_top_k: 使用检索到的第 k 个示例的 SQL 作为预测（模拟生成）\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for idx, row in test_data.iterrows():\n",
                "        question = row['question']\n",
                "        ground_truth_sql = row['sql']\n",
                "        \n",
                "        # 检索相似示例\n",
                "        search_results = collection.query(\n",
                "            query_texts=[question],\n",
                "            n_results=use_top_k\n",
                "        )\n",
                "        \n",
                "        # 使用最相似示例的 SQL 作为预测（模拟生成）\n",
                "        predicted_sql = search_results['metadatas'][0][use_top_k-1]['sql']\n",
                "        \n",
                "        # 评估\n",
                "        eval_result = evaluate_sql(predicted_sql, ground_truth_sql)\n",
                "        eval_result['question'] = question\n",
                "        eval_result['predicted'] = predicted_sql\n",
                "        eval_result['ground_truth'] = ground_truth_sql\n",
                "        eval_result['distance'] = search_results['distances'][0][use_top_k-1]\n",
                "        \n",
                "        results.append(eval_result)\n",
                "    \n",
                "    return results\n",
                "\n",
                "# 运行评估\n",
                "print(\"正在评估 Text2SQL 系统...\\n\")\n",
                "eval_results = evaluate_text2sql_system(test_df, eval_collection, use_top_k=1)\n",
                "\n",
                "# 计算整体指标\n",
                "exact_match_acc = sum(r['exact_match'] for r in eval_results) / len(eval_results)\n",
                "normalized_match_acc = sum(r['normalized_match'] for r in eval_results) / len(eval_results)\n",
                "avg_keyword_score = sum(r['keyword_score'] for r in eval_results) / len(eval_results)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"评估结果汇总\")\n",
                "print(\"=\"*80)\n",
                "print(f\"测试样本数: {len(eval_results)}\")\n",
                "print(f\"\\n准确率指标:\")\n",
                "print(f\"  - 精确匹配率: {exact_match_acc*100:.2f}%\")\n",
                "print(f\"  - 标准化匹配率: {normalized_match_acc*100:.2f}%\")\n",
                "print(f\"  - 平均关键词分数: {avg_keyword_score:.4f}\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5o6p7q8r",
            "metadata": {},
            "source": [
                "### 2.8 详细分析评估结果\n",
                "\n",
                "分析哪些类型的查询表现较好，哪些较差。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "6p7q8r9s",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "评估案例分析:\n",
                        "\n",
                        "================================================================================\n",
                        "\n",
                        "✅ 成功案例（标准化匹配）:\n",
                        "\n",
                        "\n",
                        "❌ 失败案例（未匹配）:\n",
                        "\n",
                        "1. 问题: Close (delete) store with ID 3.\n",
                        "   真实 SQL: DELETE FROM store WHERE store_id = 3;\n",
                        "   预测 SQL: DELETE FROM inventory WHERE inventory_id = 21;\n",
                        "   关键词分数: 0.6000\n",
                        "   检索距离: 0.7824\n",
                        "\n",
                        "2. 问题: Create a new customer for store 1 named 'Alice Brown'.\n",
                        "   真实 SQL: INSERT INTO customer (store_id, first_name, last_name, create_date, address_id, active) VALUES (1, 'Alice', 'Brown', NOW(), 1, 1);\n",
                        "   预测 SQL: INSERT INTO staff (first_name, last_name, address_id, store_id, active, username) VALUES ('Bob', 'Lee', 1, 1, 1, 'boblee');\n",
                        "   关键词分数: 0.3333\n",
                        "   检索距离: 0.6944\n",
                        "\n",
                        "3. 问题: Change payment amount of payment ID 6 to 12.50.\n",
                        "   真实 SQL: UPDATE payment SET amount = 12.50 WHERE payment_id = 6;\n",
                        "   预测 SQL: DELETE FROM payment WHERE payment_id = 7;\n",
                        "   关键词分数: 0.4000\n",
                        "   检索距离: 0.8397\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# 展示一些评估案例\n",
                "print(\"评估案例分析:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 展示成功案例\n",
                "print(\"\\n✅ 成功案例（标准化匹配）:\\n\")\n",
                "success_cases = [r for r in eval_results if r['normalized_match']]\n",
                "for i, case in enumerate(success_cases[:3]):\n",
                "    print(f\"{i+1}. 问题: {case['question']}\")\n",
                "    print(f\"   真实 SQL: {case['ground_truth']}\")\n",
                "    print(f\"   预测 SQL: {case['predicted']}\")\n",
                "    print(f\"   检索距离: {case['distance']:.4f}\\n\")\n",
                "\n",
                "# 展示失败案例\n",
                "print(\"\\n❌ 失败案例（未匹配）:\\n\")\n",
                "failure_cases = [r for r in eval_results if not r['normalized_match']]\n",
                "for i, case in enumerate(failure_cases[:3]):\n",
                "    print(f\"{i+1}. 问题: {case['question']}\")\n",
                "    print(f\"   真实 SQL: {case['ground_truth']}\")\n",
                "    print(f\"   预测 SQL: {case['predicted']}\")\n",
                "    print(f\"   关键词分数: {case['keyword_score']:.4f}\")\n",
                "    print(f\"   检索距离: {case['distance']:.4f}\\n\")\n",
                "\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7q8r9s0t",
            "metadata": {},
            "source": [
                "### 2.9 按SQL类型分析性能"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "8r9s0t1u",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "按 SQL 类型的性能分析:\n",
                        "\n",
                        "================================================================================\n",
                        "DELETE 类型 (样本数: 2):\n",
                        "  - 精确匹配率: 0.0%\n",
                        "  - 标准化匹配率: 0.0%\n",
                        "  - 平均关键词分数: 0.6000\n",
                        "\n",
                        "INSERT 类型 (样本数: 2):\n",
                        "  - 精确匹配率: 0.0%\n",
                        "  - 标准化匹配率: 0.0%\n",
                        "  - 平均关键词分数: 0.3333\n",
                        "\n",
                        "UPDATE 类型 (样本数: 2):\n",
                        "  - 精确匹配率: 0.0%\n",
                        "  - 标准化匹配率: 0.0%\n",
                        "  - 平均关键词分数: 0.4500\n",
                        "\n",
                        "SELECT 类型 (样本数: 2):\n",
                        "  - 精确匹配率: 0.0%\n",
                        "  - 标准化匹配率: 0.0%\n",
                        "  - 平均关键词分数: 0.3500\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# 按 SQL 类型分析性能\n",
                "print(\"按 SQL 类型的性能分析:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "sql_type_performance = {}\n",
                "for result in eval_results:\n",
                "    # 提取 SQL 类型\n",
                "    sql_type = result['ground_truth'].split()[0].upper()\n",
                "    \n",
                "    if sql_type not in sql_type_performance:\n",
                "        sql_type_performance[sql_type] = {\n",
                "            'count': 0,\n",
                "            'exact_match': 0,\n",
                "            'normalized_match': 0,\n",
                "            'keyword_scores': []\n",
                "        }\n",
                "    \n",
                "    sql_type_performance[sql_type]['count'] += 1\n",
                "    sql_type_performance[sql_type]['exact_match'] += result['exact_match']\n",
                "    sql_type_performance[sql_type]['normalized_match'] += result['normalized_match']\n",
                "    sql_type_performance[sql_type]['keyword_scores'].append(result['keyword_score'])\n",
                "\n",
                "for sql_type, perf in sql_type_performance.items():\n",
                "    count = perf['count']\n",
                "    if count == 0:\n",
                "        continue\n",
                "    exact_acc = perf['exact_match'] / count * 100\n",
                "    norm_acc = perf['normalized_match'] / count * 100\n",
                "    avg_keyword = sum(perf['keyword_scores']) / count\n",
                "    \n",
                "    print(f\"{sql_type} 类型 (样本数: {count}):\")\n",
                "    print(f\"  - 精确匹配率: {exact_acc:.1f}%\")\n",
                "    print(f\"  - 标准化匹配率: {norm_acc:.1f}%\")\n",
                "    print(f\"  - 平均关键词分数: {avg_keyword:.4f}\\n\")\n",
                "\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9s0t1u2v",
            "metadata": {},
            "source": [
                "### 2.10 总结与改进方向\n",
                "\n",
                "**当前系统特点：**\n",
                "- 使用向量检索选择最相似的示例\n",
                "- 直接使用检索到的示例 SQL 作为预测（模拟零样本预测）\n",
                "\n",
                "**评估体系的优势：**\n",
                "1. ✅ 多维度评估指标（精确匹配、标准化匹配、关键词匹配）\n",
                "2. ✅ 按 SQL 类型细分的性能分析\n",
                "3. ✅ 详细的案例分析（成功/失败案例）\n",
                "\n",
                "**改进方向：**\n",
                "1. **集成真实 LLM**: 使用 GPT-4/Claude 等模型基于检索到的示例生成 SQL\n",
                "2. **增强示例检索**: \n",
                "   - 使用重排序模型提高检索质量\n",
                "   - 考虑 SQL 结构相似性（不仅仅是问题相似性）\n",
                "3. **数据库 Schema 增强**: 将表结构信息加入 Prompt\n",
                "4. **执行验证**: 在测试数据库上实际执行 SQL，验证结果正确性\n",
                "5. **多样性采样**: 检索多个不同类型的示例提供更全面的上下文\n",
                "6. **错误分析**: 构建错误类型分类器，识别常见错误模式\n",
                "\n",
                "**实际应用建议：**\n",
                "- 对于生产环境，建议标准化匹配率达到 80% 以上\n",
                "- 关键词分数可用于识别部分正确的查询\n",
                "- 结合人工审核处理低置信度的预测\n",
                "\n",
                "---\n",
                "\n",
                "## 🎉 作业完成总结\n",
                "\n",
                "本次作业我们完成了两个重要任务：\n",
                "\n",
                "### 作业1：检索结果后处理方法 ✅\n",
                "- **重排技术**: Cross-Encoder 重排、MMR 多样性重排\n",
                "- **压缩技术**: 相关性过滤、上下文压缩、文本摘要\n",
                "- **校正技术**: 查询扩展、查询重写、混合检索\n",
                "- **综合应用**: 完整检索管道、方法对比\n",
                "\n",
                "### 作业2：Text2SQL 评估体系 ✅\n",
                "- **数据处理**: 加载和分析 Sakila 数据集\n",
                "- **向量检索**: 构建 Few-Shot 示例检索系统\n",
                "- **评估指标**: 精确匹配、标准化匹配、关键词匹配\n",
                "- **完整评估**: 数据集划分、评估流程、结果分析\n",
                "\n",
                "### 核心收获\n",
                "1. 掌握了多种检索后处理技术及其组合使用\n",
                "2. 了解了 Text2SQL 任务的评估方法和关键指标\n",
                "3. 学会了构建完整的评估流程和性能分析体系\n",
                "4. 理解了向量检索在Few-Shot学习中的应用"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rag-project01",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
